version: "3"

services:
  coordinator:
    image: docker-hadoop-coordinator:1.0.0
    container_name: "cluster-initializer"
    volumes:
      - ./coordinator/run_cluster.sh:/run_cluster.sh
      - ./base/functions.sh:/functions.sh
    entrypoint: /run_cluster.sh
    networks:
      hadoop.net:

  # Active namenode
  namenode1:
    image: docker-hadoop-namenode:1.0.0
    container_name: namenode1
    hostname: namenode1
    restart: always
    ports:
      - 9870:9870
    volumes:
      - ./namenode/run_active_nn.sh:/scripts/run_active_nn.sh
      # - ./namenode/nn1/100_initialize.sh:/postrun/100_initialize.sh
    environment:
      CLUSTER_NAME: nameservice
      # SERVICE_PRECONDITION: "journal1.hadoop:8485 journal2.hadoop:8485 journal3.hadoop:8485 zk1.hadoop:2181 zk2.hadoop:2181 zk3.hadoop:2181"
    env_file:
      - ./hadoop.env
    networks:
      hadoop.net:
        aliases:
          - namenode1
  
  # Standby namenode, resourcemanager, yarn historyserver, sparkhistoryserver
  namenode2:
    image: docker-hadoop-namenode:1.0.0
    container_name: namenode2
    hostname: namenode2
    restart: always
    ports:
      - 9871:9870
      - 8088:8088
      - 8188:8188
      - 18080:18080
    volumes:
      - ./namenode/run_standby_nn.sh:/scripts/run_standby_nn.sh
      - ./namenode/run_rm.sh:/scripts/run_rm.sh
      - ./namenode/run_yarn_hs.sh:/scripts/run_yarn_hs.sh
      - ./namenode/initialize.sh:/scripts/initialize.sh
      - ./spark/run_spark_hs.sh:/scripts/run_spark_hs.sh
      - ./spark/spark_history_server.conf:/spark_history_server.conf
      - ./initialization/:/initialization/
    environment:
      CLUSTER_NAME: nameservice
      # SERVICE_PRECONDITION: "journal1.hadoop:8485 journal2.hadoop:8485 journal3.hadoop:8485 zk1.hadoop:2181 zk2.hadoop:2181 zk3.hadoop:2181 namenode1:9000"
    env_file:
      - ./hadoop.env
    networks:
      hadoop.net:
        aliases:
          - namenode2
          - resourcemanager
          - spark-historyserver
          - yarn-historyserver
  # Each datanode has NodeManager in same container
  # Also, There are additional services in the datanode,
  # datanode1: Zookeeper, Journal Node, 
  # datanode2: Zookeeper, Journal Node, 
  # datanode3: Zookeeper, Journal Node, 
  # datanode4: Hive Thrift server, Hive Metastore
  # datanode5: Hue
  # datanode6: Spark ThriftServer
  # datanode7: 
  datanode1:
    image: docker-hadoop-hive:1.0.0
    container_name: datanode1
    hostname: datanode1
    restart: always
    volumes:
      - ./datanode/run_datanode.sh:/scripts/run_datanode.sh
      - ./datanode/run_zookeeper.sh:/scripts/run_zookeeper.sh
      - ./datanode/run_journal.sh:/scripts/run_journal.sh  
      - ./datanode/run_nodemanager.sh:/scripts/run_nodemanager.sh
      - ./hive/hive_config.sh:/config/hive_config.sh
      - ./datanode/zoo.cfg:/opt/zookeeper/conf/zoo.cfg
      - ./spark/spark-bin:/opt/spark
    environment:
      MY_NODE_NUM: "1"
    env_file:
      - ./hadoop.env
      - ./hive/hadoop-hive.env
    ports:
      - 9861:9864
      - 8041:8042
    networks:
      hadoop.net:
        aliases:
          - zk1.hadoop
          - journal1.hadoop

  datanode2:
    image: docker-hadoop-hive:1.0.0
    container_name: datanode2
    hostname: datanode2
    restart: always
    volumes:
      - ./datanode/run_datanode.sh:/scripts/run_datanode.sh
      - ./datanode/run_zookeeper.sh:/scripts/run_zookeeper.sh
      - ./datanode/run_journal.sh:/scripts/run_journal.sh  
      - ./datanode/run_nodemanager.sh:/scripts/run_nodemanager.sh
      - ./hive/hive_config.sh:/config/hive_config.sh
      - ./datanode/zoo.cfg:/opt/zookeeper/conf/zoo.cfg
      - ./spark/spark-bin:/opt/spark
    environment:
      MY_NODE_NUM: "2"
    env_file:
      - ./hadoop.env
      - ./hive/hadoop-hive.env
    ports:
      - 9862:9864
      - 8042:8042
    networks:
      hadoop.net:
        aliases:
          - zk2.hadoop
          - journal2.hadoop
  
  datanode3:
    image: docker-hadoop-hive:1.0.0
    container_name: datanode3
    hostname: datanode3
    restart: always
    volumes:
      - ./datanode/run_datanode.sh:/scripts/run_datanode.sh
      - ./datanode/run_zookeeper.sh:/scripts/run_zookeeper.sh
      - ./datanode/run_journal.sh:/scripts/run_journal.sh  
      - ./datanode/run_nodemanager.sh:/scripts/run_nodemanager.sh
      - ./hive/hive_config.sh:/config/hive_config.sh
      - ./datanode/zoo.cfg:/opt/zookeeper/conf/zoo.cfg
      - ./spark/spark-bin:/opt/spark
    environment:
      MY_NODE_NUM: "3"
    env_file:
      - ./hadoop.env
      - ./hive/hadoop-hive.env
    ports:
      - 9863:9864
      - 8043:8042
    networks:
      hadoop.net:
        aliases:
          - zk3.hadoop
          - journal3.hadoop
  
  datanode4:
    image: docker-hadoop-hive:1.0.0
    container_name: datanode4
    hostname: datanode4
    restart: always
    volumes:
      - ./datanode/run_datanode.sh:/scripts/run_datanode.sh
      - ./datanode/run_nodemanager.sh:/scripts/run_nodemanager.sh
      - ./hive/run_hive_metastore.sh:/scripts/run_hive_metastore.sh
      - ./hive/run_hive_server.sh:/scripts/run_hive_server.sh
      - ./hive/hive_config.sh:/config/hive_config.sh
      - ./spark/spark-bin:/opt/spark
    env_file:
      - ./hadoop.env
      - ./hive/hadoop-hive.env
    ports:
      - 9864:9864
      - 8044:8042
      - 10000:10000
      - 10001:10001
      - 10002:10002
      - 9083:9083
    networks:
      hadoop.net:
        aliases:
          - hive-server
          - hive-metastore
  
  datanode5:
    image: docker-hadoop-hive:1.0.0
    container_name: datanode5
    hostname: datanode5
    restart: always
    volumes:
      - ./datanode/run_datanode.sh:/scripts/run_datanode.sh
      - ./datanode/run_nodemanager.sh:/scripts/run_nodemanager.sh
      - ./hive/hive_config.sh:/config/hive_config.sh
      - ./spark/spark-bin:/opt/spark
    env_file:
      - ./hadoop.env
      - ./hive/hadoop-hive.env
    ports:
      - 9865:9864
      - 8045:8042
    networks:
      hadoop.net:
  
  datanode6:
    image: docker-hadoop-hive:1.0.0
    container_name: datanode6
    hostname: datanode6
    restart: always
    volumes:
      - ./datanode/run_datanode.sh:/scripts/run_datanode.sh
      - ./datanode/run_nodemanager.sh:/scripts/run_nodemanager.sh
      - ./spark/run_spark_thrift_server.sh:/scripts/run_spark_thrift_server.sh
      - ./hive/hive_config.sh:/config/hive_config.sh
      - ./spark/spark-bin:/opt/spark
    env_file:
      - ./hadoop.env
      - ./hive/hadoop-hive.env
    ports:
      - 9866:9864
      - 8046:8042
      - 4040:4040
    networks:
      hadoop.net:
        aliases:
          - spark-adhoc

  datanode7:
    image: docker-hadoop-hive:1.0.0
    container_name: datanode7
    hostname: datanode7
    restart: always
    volumes:
      - ./datanode/run_datanode.sh:/scripts/run_datanode.sh
      - ./datanode/run_nodemanager.sh:/scripts/run_nodemanager.sh
      - ./hive/hive_config.sh:/config/hive_config.sh
      - ./spark/spark-bin:/opt/spark
    env_file:
      - ./hadoop.env
      - ./hive/hadoop-hive.env
    ports:
      - 9867:9864
      - 8047:8042
    networks:
      hadoop.net:

  cluster-db:
    image: postgres:13.1
    container_name: cluster-db
    hostname: cluster-db
    environment:
      POSTGRES_PASSWORD: "postgres"
      POSTGRES_HOST_AUTH_METHOD: "trust"
    networks:
      - hadoop.net
    ports:
      - "5432:5432"
    volumes:
      - ./hive/sql/create_db.sql:/docker-entrypoint-initdb.d/create_hive_db.sql
      - ./hue/sql/create_db.sql:/docker-entrypoint-initdb.d/create_hue_db.sql
  
  # hue:
  #   image: docker-hadoop-hue:1.0.0
  #   container_name: hue
  #   hostname: hue
  #   env_file:
  #     - ./hive/hadoop-hive.env
  #     - ./hadoop.env
  #   environment:
  #     SERVICE_PRECONDITION: "hive-server:10000"
  #   networks:
  #     - hadoop.net
  #   volumes:
  #     - ./spark/spark-bin:/opt/spark
  #   ports:
  #     - "8888:8888"

  # # trino:
  # #   image: docker-hadoop-trino:1.0.0
  # #   container_name: trino
  # #   hostname: trino
  # #   env_file:
  # #     - ./hive/hadoop-hive.env
  # #     - ./hadoop.env
  # #   environment:
  # #     SERVICE_PRECONDITION: "hive-server:10000"
  # #   networks:
  # #     - hadoop.net

networks:
  hadoop.net:
    external: true